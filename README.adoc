// header file for arc42-template,
// including all help texts
//
// ====================================


// configure DE settings for asciidoc
// asciidoc settings for DE (German)
// ==================================
// toc-title definition MUST follow document title without blank line!
:toc-title: Inhaltsverzeichnis

// enable table-of-contents
:toc:

:caution-caption: Achtung
:important-caption: Wichtig
:note-caption: Hinweis
:tip-caption: Tip
:warning-caption: Warnung

:appendix-caption: Anhang
:example-caption: Beispiel
:figure-caption: Abbildung
:table-caption: Tabelle

// where are images located?
:imagesdir: ./documentation/images

= image:arc42-logo.png[arc42] Template
:revnumber: 8.2 DE
:revdate: Januar 2023
:revremark: (basiert auf AsciiDoc Version)
// toc-title definition MUST follow document title without blank line!
:toc-title: Inhaltsverzeichnis

//additional style for arc42 help callouts
++++
<style>
.arc42help {font-size:small; width: 14px; height: 16px; overflow: hidden; position: absolute; right: 0; padding: 2px 0 3px 2px;}
.arc42help::before {content: "?";}
.arc42help:hover {width:auto; height: auto; z-index: 100; padding: 10px;}
.arc42help:hover::before {content: "";}
@media print {
	.arc42help {display:none;}
}
</style>
++++


:homepage: https://arc42.org

:keywords: software-architecture, documentation, template, arc42

:numbered!:

**Über arc42**

[role="lead"]
arc42, das Template zur Dokumentation von Software- und Systemarchitekturen.

Template Version {revnumber}. {revremark}, {revdate}

Created, maintained and (C) by Dr. Peter Hruschka, Dr. Gernot Starke and contributors.
Siehe https://arc42.org.


// horizontal line
***




// numbering from here on
:numbered:

<<<<
// 1. Anforderungen und Ziele

[[section-introduction-and-goals]]
==	Einführung und Ziele



=== Aufgabenstellung



=== Qualitätsziele



=== Stakeholder



[cols="1,1,2" options="header"]
|===
|Rolle |Kontakt |Erwartungshaltung
| _<Rolle-1>_ | _<Kontakt-1>_ | _<Erwartung-1>_
| _<Rolle-2>_ | _<Kontakt-2>_ | _<Erwartung-2>_
|===

<<<<
// 2. Randbedingungen

[[section-architecture-constraints]]
== Randbedingungen



<<<<
// 3. Kontextabgrenzung

[[section-system-scope-and-context]]
== Kontextabgrenzung



=== Fachlicher Kontext



**<Diagramm und/oder Tabelle>**

**<optional: Erläuterung der externen fachlichen Schnittstellen>**

=== Technischer Kontext



**<Diagramm oder Tabelle>**

**<optional: Erläuterung der externen technischen Schnittstellen>**

**<Mapping fachliche auf technische Schnittstellen>**

<<<<
// 4. Lösungsstrategie

[[section-solution-strategy]]
== Lösungsstrategie


=== Technologieentscheidungen
image:drawio-architecture-overview.png[architecture-overview]

==== Backend: ASP.NET
Da ich die meiste Erfahrung mit dem .NET Stack habe, habe ich mich für diesen Stack entschieden.
Da das Team am meisten
Außerdem ist das ASP.NET Framework speziell für die Entwicklung von Web APIs entwickelt worden und bringt deswegen alle notwendigen Funktionen mit sich.

==== Frontend: Angular
Im Frontend wird eine Single-Page-Anwendung gebaut.
Daher schien Angular aufgrund der komponentenbasierten Entwicklung und dem dynamischen Nachladen als gute Wahl.
Angular ist ein sehr umfangreiches Framework (vielleicht zu umfangreich für dieses Projekt), aber mit ein wenig Erfahrung ist es schnell.

==== Datenbank: MongoDB
Der Hauptgrund für die Verwendung der Document Database MongoDB ist, dass die externe API eine JSON-Datei zurückgibt.
Strukturierte Datenformate wie JSON können in MongoDB gut gespeichert werden.
Außerdem kann MongoDB mit dem Testcontainers-Projekt containerisiert werden, was das Schreiben von Integrationstests vereinfacht.
Zuletzt wird MongoDB durch ein NuGet Paket in .NET unterstützt, was das Schreiben der Anfragen vereinfacht.


=== Top-Level-Zerlegung des Systems

**3 Schichten Architektur**

==== NHTSA Vehicle API
Als externe API wird die Vehicle API von der US Behörde für Straßen und Fahrzeugsicherheit (NHTSA) eingebunden.
Über die API kann eine umfassende Liste von Fahrzeugmarken- und Modellen abgerufen werden.
Die API wurde ausgewählt, weil sie im Vergleich zu anderen Fahrzeugdaten-APIs kostenlos und frei verfügbar ist (kein API-Key notwendig).
Dafür enthält die API jedoch keine Daten zu den Fahrzeugpreisen.



=== Entscheidungen zur Erreichung der wichtigsten Qualitätsanforderungen

==== Functional Suitability
- link:https://github.com/studgantfe7623/CarCore/blob/main/app/Carcore.Test/UnitTest.cs[Unit Tests] mit Mocking
- link:https://github.com/studgantfe7623/CarCore/blob/main/app/Carcore.Test/IntegrationsTestAPI.cs[Integration Tests] schießen gegen die API und testen die Anwendung von Kopf bis Fuß.
Die Datenbank wird mit link:https://dotnet.testcontainers.org/[Testcontainers] hochgefahren, damit den Tests auch in der CI Pipeline eine Datenbank zur Verfügung steht.


==== Sicherheit
- Dependabot: hält Packages aktuell um Sicherheitslücken zu vermeiden.
Es werden automatisch Emails verschickt, wenn Pakete Sicherheitslücken aufweisen.
Die Pakete müssen dann manuell aktualisiert werden.
- link:https://github.com/studgantfe7623/CarCore/blob/main/.github/workflows/owasp-zap.yml[OWASP ZAP] scannt die API auf Security Vulnerabilities mittels der Swagger Openapi Definition.
Der Scan ist als GitHub Action umgesetzt und wird bei jedem Check-In durchgeführt.
Ergebnis: 99 Bestanden, 2 Warnings.
Für detailliertere Informationen siehe link:https://github.com/studgantfe7623/CarCore/blob/main/documentation/appendix/zap-scan-report.md[Report].


==== Benutzeroberfläche
- link:https://github.com/studgantfe7623/CarCore/blob/main/frontend/cypress/e2e/spec.cy.ts[Cypress]:
E2E Tests in Angular (Acceptance Test-driven Development)


==== Maintainability
- link:https://github.com/studgantfe7623/CarCore/blob/main/.github/workflows/sonarcloud.yml[Build Pipeline]  via GitHub Actions baut die Anwendung bei jedem Git Check-In. Falls der Build-Vorgang fehschlägt, zeugt das die Pipeline an.

==== Statische Codeanalyse
- link:https://sonarcloud.io/project/overview?id=studgantfe7623_CarCore[Sonarcloud]: Statische Codeanalyse +
Übersicht über die SonarCloud Ergebnisse:
// Zero Validations Policy wird eingehalten
image:sonarcloud.png[arc42]
- https://github.com/coverlet-coverage/coverlet[Coverlet] erstellt Test Coverage Datei innerhalb der Build-Pipeline.
Test Coverage Datei wird in Sonarcloud hochgeladen.


==== Infrastruktur (Docker)
Das Dockerfile wurde auf Security Risiken untersucht

- link:https://github.com/hadolint/hadolint[hadolint]
- link:https://github.com/projectatomic/dockerfile_lint[dockerfile-lint]: Das Dockerfile wurde auf die
link:basic_rules.yaml[basic_rules], link:recommended_label_rules.yaml[recommended_label_rules] und link:https://github.com/projectatomic/dockerfile_lint/blob/master/sample_rules/security_rules.yaml[security_rules] aus dem offiziellen dockerfile_lint Repository geprüft.
Anhand der Empfehlungen wurden Labels ergänzt.
// und es wurde sichergestellt, dass der Container nicht als Root User ausgeführt wird.


==== Performance
Um das Qualitätskriterium Performance zu testen wurde das Performance Testing Tool K6 verwendet.
Das Performance Testing beschränkt sich auf die HTTP-GET Methoden, da für die HTTP-POST Methoden eine Payload hätte generiert werden müssen.
Für einen ersten Test der Performance erschien dieser Mehraufwand nicht gerechtfertigt.
Um die Performance des Systems zu bestimmen, wurden Lasttests, Stresstests und Spike-Tests durchgeführt.

Lasttest::
Mit den Lasttests wurde versucht,  die aktuelle Performance des Systems in Bezug auf die Anzahl der gleichzeitigen Benutzer oder Anfragen pro Sekunde zu bewerten.
image:last-test.png[Kategorien von Qualitätsanforderungen]
Die Abbildung zeigt wie der Lasttest für die Anfwendung aufgebaut ist.
Die Anfragelast wird langsam auf 100 Benutzeranfragen pro Sekunde gesteigert.
Dieser Wert wird dann für 10 Minuten gehalten und anschließend wieder reduziert.

Ergebnis:::
- Das System beantwortet Anfragen in weniger als 4 Sekunden für das 90%-Perzentil bei 100 Anfragen/Sekunde.

Stresstest::
Stresstests wurden eingesetzt um die Grenzen des Systems zu ermitteln.
Ziel war es, die Stabilität und Zuverlässigkeit des Systems unter extremen Bedingungen zu überprüfen.
image:stress-test.png[Kategorien von Qualitätsanforderungen]
Im durchgeführten Stresstest wird die Anzahl der Anfragen pro Sekunde bis zur Belastungsgrenze und darüber hinaus gesteigert.
Im Detail wird der Maximalwert von 400 Nutzeranfragen pro Sekunde nach 28 Minuten erreicht.
Danach wird die Anforderungslast langsam reduziert, um zu sehen, ob sich das System erholt.

Ergebnisse:::
- Bei 300 gleichzeitigen Anfragen kommt es vereinzelt zu Fehlern (Zeitüberschreitungen).
- Je näher man an die 400 gleichzeitigen Benutzer kommt, desto wahrscheinlicher wird eine Zeitüberschreitung.
- Das System erholt sich wieder, wenn die Anforderungslast abnimmt.
- Die Belastungsgrenze des Systems liegt bei ca. 300 gleichzeitigen Benutzern.
- Während des Lasttests wurden insgesamt 52920 erfolgreiche und 35 fehlgeschlagene Anfragen ausgeführt.

Spike Test::
Der Spike-Test ist eine Variante des Stresstests, bei dem die Belastung nicht schrittweise erhöht wird, sondern in einem sehr kurzen Zeitfenster Spitzenwerte erreicht werden.
Stresstests wurden durchgeführt, um festzustellen, wie sich das System bei einem plötzlichen Anstieg der Anfragelast verhält.
image:spike-test.png[Kategorien von Qualitätsanforderungen]

Ergebnis:::
-  Das System reagierte schlecht.
Es produzierte Fehler während des Anfrage-Spikes, konnte sich aber erholen, nachdem der Spike nachgelassen hatte.

Threats to Validity::
Die Ergebnisse sind mit Vorsicht zu genießen, da die Lasttests in diesem Fall die Leistung des lokalen Rechners und nicht die der Anwendung testen.
Wie in der folgenden Abbildung zu sehen ist, war die CPU-Auslastung auf dem lokalen Rechner ab 100 Anfragen pro Sekunde fast immer bei 100 %.
image:cpu-load.png[cpu-load]

<<<<
// 5. Bausteinsicht

[[section-building-block-view]]
== Bausteinsicht



=== Whitebox Gesamtsystem



_**<Übersichtsdiagramm>**_

*Übersichtsdiagramm*

Übersichtsdiagramm::

Begründung:: _<Erläuternder Text>_

Enthaltene Bausteine:: _<Beschreibung der enthaltenen Bausteine (Blackboxen)>_

Wichtige Schnittstellen:: _<Beschreibung wichtiger Schnittstellen>_



==== <Name Blackbox 1>



_<Zweck/Verantwortung>_

_<Schnittstelle(n)>_

_<(Optional) Qualitäts-/Leistungsmerkmale>_

_<(Optional) Ablageort/Datei(en)>_

_<(Optional) Erfüllte Anforderungen>_

_<(optional) Offene Punkte/Probleme/Risiken>_

==== <Name Blackbox 2>

_<Blackbox-Template>_

==== <Name Blackbox n>

_<Blackbox-Template>_


==== <Name Schnittstelle 1>

...

==== <Name Schnittstelle m>

=== Ebene 2



==== Whitebox _<Baustein 1>_



_<Whitebox-Template>_

==== Whitebox _<Baustein 2>_

_<Whitebox-Template>_

...

==== Whitebox _<Baustein m>_

_<Whitebox-Template>_

=== Ebene 3



==== Whitebox <_Baustein x.1_>



_<Whitebox-Template>_

==== Whitebox <_Baustein x.2_>

_<Whitebox-Template>_

==== Whitebox <_Baustein y.1_>

_<Whitebox-Template>_

<<<<
// 6. Laufzeitsicht

[[section-runtime-view]]
== Laufzeitsicht



=== _<Bezeichnung Laufzeitszenario 1>_

*  <hier Laufzeitdiagramm oder Ablaufbeschreibung einfügen>
*  <hier Besonderheiten bei dem Zusammenspiel der Bausteine in diesem Szenario erläutern>

=== _<Bezeichnung Laufzeitszenario 2>_

...

=== _<Bezeichnung Laufzeitszenario n>_

...

<<<<
// 7. Verteilungssicht

[[section-deployment-view]]
== Verteilungssicht



=== Infrastruktur Ebene 1



_**<Übersichtsdiagramm>**_

Begründung:: _<Erläuternder Text>_

Qualitäts- und/oder Leistungsmerkmale:: _<Erläuternder Text>_

Zuordnung von Bausteinen zu Infrastruktur:: _<Beschreibung der Zuordnung>_

=== Infrastruktur Ebene 2



==== _<Infrastrukturelement 1>_

_<Diagramm + Erläuterungen>_

==== _<Infrastrukturelement 2>_

_<Diagramm + Erläuterungen>_

...

==== _<Infrastrukturelement n>_

_<Diagramm + Erläuterungen>_

<<<<
// 8. Querschnittliche Konzepte

[[section-concepts]]
== Querschnittliche Konzepte



=== _<Konzept 1>_

_<Erklärung>_

=== _<Konzept 2>_

_<Erklärung>_

...

=== _<Konzept n>_

_<Erklärung>_

<<<<
// 9. Entscheidungen

[[section-design-decisions]]
== Architekturentscheidungen


|=====
|Titel |Status |Context |Entscheidung |Konsequenzen
|.NET Core als Architektur Framework |Akzeptiert |Als Framework haben wir uns für .NET Core entschieden. Als Alternativen Standen noch Java und TypeScript zur Verfügung. |Ich habe mich für .NET entschieden, weil ich mich nur damit auskenne. |Ich befinde mich im .NET Stack und nutze die dafür vorgesehen Tools. (Beispielsweise MS Test als Testing Framework → diese "Unter"-Entscheidung muss dann nicht mehr dokumentiert werden, weil es der "Go-to" in der .NET Umgebung ist)
|MSTest als Unit Testing Framework |Akzeptiert |Als bekannte Unit Testing Frameworks gibt es in .NET NUnit, xUnit.net und MSTest. Die Frameworks unterscheiden sich in ihrer Funktionalität nur gering. |Da ich schon in der Arbeit mit MSTest gearbeitet habe und ich mich daher damit am besten aus kenne, wurde sich für MSTest als Unit Testing Framework entschieden. |Die Code-Annotationen unterscheiden sich
|=====

<<<<
// 10. Qualitätsanforderungen

[[section-quality-scenarios]]
== Qualitätsanforderungen

// Funktions- und Unit Tests
Functional Suitability::
- Die Anwendung kann Daten in eine externe Datenbank schreiben und die Daten erfolgreich daraus lesen.
// Testcontainers
- Die Anwendung kann eine API aufrufen und die erwarteten Antworten empfangen.
// Testcontainers


// Lasttests
Performance::
- Die erwartete Benutzerlast sind 100 gleichzeitige Benutzeranfragen.
- Die durchschnittliche Reaktionszeit der Anwendung bei der definierten Benutzerlast liegt unter 1 Sekunde.
- Die Anwendung ist in der Lage, 100 gleichzeitige Benutzeranfragen pro Sekunde zu verarbeiten, ohne dass die Antwortzeiten signifikant steigen.


Maintainability::
- Quellcode soll durch den Einsatz der passenden Patterns modular aufgebaut sein
- Das Code Repository wird bei jedem Checkin automatisiert gebaut
// GitHub Actions
- Alle Datenzugriffsoperationen erfolgen ausschließlich über die definierte Schnittstelle
- Alle Schnelllaufenden Tests (Unit Test, ..., halt keine End-to-end tests oder so) werden bei jedem Check-in getestet.
// GitHub Actions
- Alle Auffälligkeiten aus der statischen Codeanalyse müssen beseitigt werden (0 Violation Policy)
// Sonarcloud


Sicherheit::
- Die Anwendung soll den Entwickler über veraltete Abhängigkeiten zu NuGet Paketen informieren und diese bei Bedarf direkt aktualisieren.
// GitHub Dependabot
- Die Anwendung soll gegen die OWASP Top 10 Application Security Risks geschützt sein
// OWASP DependencyCheck
- Es werden ausschließliche sichere Kommunikationsprotokolle verwendet (HTTPS statt HTTP)


Weitere Qualitätsanforderungen: Reliability, Skalierbarkeit


.Weiterführende Informationen

Siehe https://docs.arc42.org/section-10/[Qualitätsanforderungen] in der online-Dokumentation (auf Englisch!).

=== Qualitätsbaum



=== Qualitätsszenarien



<<<<
// 11. Risiken

[[section-technical-risks]]
== Risiken und technische Schulden




<<<<
// 12. Glossar

[[section-glossary]]
== Glossar



[cols="e,2e" options="header"]
|===
|Begriff |Definition

|<Begriff-1>
|<Definition-1>

|<Begriff-2
|<Definition-2>
|===
